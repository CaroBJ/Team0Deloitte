{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T10:30:01.699171Z",
     "iopub.status.busy": "2023-12-21T10:30:01.698784Z",
     "iopub.status.idle": "2023-12-21T10:30:09.238625Z",
     "shell.execute_reply": "2023-12-21T10:30:09.237534Z",
     "shell.execute_reply.started": "2023-12-21T10:30:01.699139Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "#from torchmetrics.functional import dice\n",
    "from glob import glob\n",
    "# import cv2\n",
    "#from torchmetrics.classification import Dice\n",
    "#from torchmetrics.classification import BinaryF1Score\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "#import wandb used to parameter tuning\n",
    "#from torchvision.transforms import v2\n",
    "import time\n",
    "from torchmetrics import JaccardIndex\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T10:30:09.241395Z",
     "iopub.status.busy": "2023-12-21T10:30:09.240860Z",
     "iopub.status.idle": "2023-12-21T10:30:09.283770Z",
     "shell.execute_reply": "2023-12-21T10:30:09.282935Z",
     "shell.execute_reply.started": "2023-12-21T10:30:09.241361Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "torch.manual_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T10:30:09.285363Z",
     "iopub.status.busy": "2023-12-21T10:30:09.285069Z",
     "iopub.status.idle": "2023-12-21T10:30:09.296908Z",
     "shell.execute_reply": "2023-12-21T10:30:09.295949Z",
     "shell.execute_reply.started": "2023-12-21T10:30:09.285331Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CarSegData(Dataset):\n",
    "    def __init__(self, data_root, transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.class_labels = {\n",
    "            10: 1,\n",
    "            20: 2,\n",
    "            30: 3,\n",
    "            40: 4,\n",
    "            50: 5,\n",
    "            60: 6,\n",
    "            70: 7,\n",
    "            80: 8,\n",
    "            90: 0\n",
    "        }\n",
    "        self.classes = {\n",
    "            1: \"hood\",\n",
    "            2: \"front door\",\n",
    "            3: \"rear door\",\n",
    "            4: \"frame\",\n",
    "            5: \"rear quarter panel\",\n",
    "            6: \"trunk lid\",\n",
    "            7: \"fender\",\n",
    "            8: \"bumper\",\n",
    "            9: \"rest of car\"\n",
    "        }\n",
    "\n",
    "        # List all the array files in the 'arrays' directory\n",
    "        self.array_files = np.load(data_root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.array_files)[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        array_data = self.array_files[idx,:,:,:]\n",
    "        image_data = array_data[:,:,:3]\n",
    "        target_data = array_data[:,:,3]\n",
    "\n",
    "        # Convert target data to class labels\n",
    "        target_data = self.map_to_class_labels(target_data)\n",
    "        target_data = self.map_to_classes(target_data)\n",
    "\n",
    "        # Convert to PIL image\n",
    "        image = Image.fromarray(image_data.astype('uint8'))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target_data\n",
    "\n",
    "    def map_to_classes(self, target_data):\n",
    "        class_labels = np.zeros_like(target_data)\n",
    "        for class_value, class_name in self.classes.items():\n",
    "            class_labels[target_data == class_value] = class_value\n",
    "        return torch.from_numpy(class_labels)\n",
    "    \n",
    "    def map_to_class_labels(self, target_data):\n",
    "        class_labels = np.zeros_like(target_data)\n",
    "        for old_label, new_label in self.class_labels.items():\n",
    "            class_labels[target_data == old_label] = new_label\n",
    "        return torch.from_numpy(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T10:30:09.314311Z",
     "iopub.status.busy": "2023-12-21T10:30:09.313722Z",
     "iopub.status.idle": "2023-12-21T10:30:09.331921Z",
     "shell.execute_reply": "2023-12-21T10:30:09.330937Z",
     "shell.execute_reply.started": "2023-12-21T10:30:09.314279Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_channels, out_channels,device, kernel_size = 4, stride = 2, padding = 1, norm = True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, device = device)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=False) #The choice 0.2 is from the paper\n",
    "        \n",
    "        self.use_norm= norm\n",
    "        if norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels, device = device)\n",
    "        else:\n",
    "            self.bn = None\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.use_norm:\n",
    "            x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, device,\n",
    "                kernel_size = 4, stride = 2, padding = 1,dropout = False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels,out_channels,kernel_size, stride, padding,device = device)\n",
    "    \n",
    "        self.act = nn.ReLU(inplace = False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, device = device)\n",
    "    \n",
    "\n",
    "        if dropout is not None:\n",
    "            self.dropout = nn.Dropout2d(p = 0.5, inplace = False) # p = 0.5 is from the paper\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T10:30:09.333665Z",
     "iopub.status.busy": "2023-12-21T10:30:09.333400Z",
     "iopub.status.idle": "2023-12-21T10:30:09.348957Z",
     "shell.execute_reply": "2023-12-21T10:30:09.347860Z",
     "shell.execute_reply.started": "2023-12-21T10:30:09.333640Z"
    }
   },
   "outputs": [],
   "source": [
    "#Encoder\n",
    "#C64-C128-C256-C512-C512-C512-C512-C512\n",
    "# 1    2    3    4    5    6   7    8\n",
    "#CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "class UNet_Generator(nn.Module):\n",
    "    def __init__(self,device, input_channels= 3, out_channels = 9):\n",
    "        super().__init__()\n",
    "        #Encoder\n",
    "        self.EB1 = EncoderBlock(input_channels,64,  norm=False, device = device)\n",
    "        self.EB2 = EncoderBlock(64,128, device = device)\n",
    "        self.EB3 = EncoderBlock(128,256, device = device)\n",
    "        self.EB4 = EncoderBlock(256,512, device = device)\n",
    "        self.EB5 = EncoderBlock(512,512, device = device)\n",
    "        self.EB6 = EncoderBlock(512,512, device = device)\n",
    "        self.EB7 = EncoderBlock(512,512, device = device)\n",
    "        self.EB8 = EncoderBlock(512,512, norm = False, device = device)\n",
    "        \n",
    "        #Decoder\n",
    "        self.DB8 = DecoderBlock(512,512,dropout=True, device = device)\n",
    "        self.DB7 = DecoderBlock(2*512,512,dropout=True, device = device)\n",
    "        self.DB6 = DecoderBlock(2*512,512,dropout=True, device = device)\n",
    "        self.DB5 = DecoderBlock(2*512,512,device = device)\n",
    "        self.DB4 = DecoderBlock(2*512,256,device = device)\n",
    "        self.DB3 = DecoderBlock(2*256,128,device = device)\n",
    "        self.DB2 = DecoderBlock(2*128,64,device = device)\n",
    "        self.DB1 = nn.ConvTranspose2d(2*64, out_channels, kernel_size=4, stride=2, padding=1, device= device)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #Encoder\n",
    "        e1 = self.EB1(x)\n",
    "        e2 = self.EB2(e1)\n",
    "        e3 = self.EB3(e2)\n",
    "        e4 = self.EB4(e3)\n",
    "        e5 = self.EB5(e4)\n",
    "        e6 = self.EB6(e5)\n",
    "        e7 = self.EB7(e6)\n",
    "        e8 = self.EB8(e7)\n",
    "        \n",
    "        #Decoder\n",
    "        s8 = self.DB8(e8)\n",
    "        s7 = self.DB7(torch.cat([s8,e7], dim = 1)) #Add skip connections\n",
    "        s6 = self.DB6(torch.cat([s7,e6], dim = 1)) #Add skip connections\n",
    "        s5 = self.DB5(torch.cat([s6,e5], dim = 1)) #Add skip connections\n",
    "        s4 = self.DB4(torch.cat([s5,e4], dim = 1)) #Add skip connections\n",
    "        s3 = self.DB3(torch.cat([s4,e3], dim = 1)) #Add skip connections\n",
    "        s2 = self.DB2(torch.cat([s3,e2], dim = 1))  #Add skip connections\n",
    "        s1 = self.DB1(torch.cat([s2,e1], dim = 1)) #Add skip connections\n",
    "                \n",
    "        return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T10:30:09.350512Z",
     "iopub.status.busy": "2023-12-21T10:30:09.350240Z",
     "iopub.status.idle": "2023-12-21T10:30:09.364226Z",
     "shell.execute_reply": "2023-12-21T10:30:09.363479Z",
     "shell.execute_reply.started": "2023-12-21T10:30:09.350488Z"
    }
   },
   "outputs": [],
   "source": [
    "#C64-C128-C256-C512\n",
    "class DiscriminatorBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_channels, out_channels,device, kernel_size = 4, stride = 2, padding = 1, norm = True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, device = device)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True) #The choice 0.2 is from the paper\n",
    "        \n",
    "        self.use_norm= norm\n",
    "        if norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels, device = device)\n",
    "        else:\n",
    "            self.bn = None\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_norm:\n",
    "            x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class PatchGan_Discriminator(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.l1 = DiscriminatorBlock(3 + 1,64,norm=False, device= device)\n",
    "        self.l2 = DiscriminatorBlock(64,128, device= device)\n",
    "        self.l3 = DiscriminatorBlock(128,256, device= device)\n",
    "        self.l4 = DiscriminatorBlock(256,512, device= device)\n",
    "        self.l5 = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1, device= device)\n",
    "    \n",
    "    def forward(self,mask,image):\n",
    "        #The discrimator is condition on the true image\n",
    "        if mask.shape[1] > 1:\n",
    "            mask = masker(mask)\n",
    "        x = torch.cat([mask,image], dim = 1)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        # Last output will be a value between 0 and 1\n",
    "        x = torch.sigmoid(x) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T10:30:09.365649Z",
     "iopub.status.busy": "2023-12-21T10:30:09.365350Z",
     "iopub.status.idle": "2023-12-21T10:30:09.385516Z",
     "shell.execute_reply": "2023-12-21T10:30:09.384676Z",
     "shell.execute_reply.started": "2023-12-21T10:30:09.365621Z"
    }
   },
   "outputs": [],
   "source": [
    "def masker(fake_mask):\n",
    "    fake_mask = fake_mask.float()\n",
    "    fake_mask = torch.softmax(fake_mask, dim = 1)\n",
    "    N,_,H,W = fake_mask.shape\n",
    "    return torch.argmax(fake_mask, dim=1).reshape(N,1,H,W)\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha=[1.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0], gamma=2.0):\n",
    "    ce_loss = F.cross_entropy(y_pred, y_true, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    class_weights = torch.tensor(alpha).to(y_true.device)\n",
    "    class_weights = torch.softmax(class_weights, dim = 0)\n",
    "    alpha_t = class_weights[y_true.data.view(-1).long()].view_as(y_true)\n",
    "    focal_loss = (alpha_t * (1 - pt) ** gamma * ce_loss).mean()\n",
    "    return focal_loss\n",
    "\n",
    "def generator_loss(d_out,fake_mask, true_mask, lambda_ = 100, loss = \"CE\", with_logits = False):\n",
    "    assert loss in [\"L1\",\"CE\", \"FL\",\"WCE\"]\n",
    "    #We \"trick\" the discrimintator\n",
    "    fake_target = torch.ones_like(d_out)\n",
    "    if with_logits:\n",
    "        loss_fn = F.binary_cross_entropy_with_logits\n",
    "    else:\n",
    "        loss_fn = F.binary_cross_entropy\n",
    "    adv_loss = loss_fn(d_out,fake_target)\n",
    "    if loss == \"L1\":\n",
    "        # Change 10 dim to 1 (choose labels)\n",
    "        fake_mask = masker(fake_mask)\n",
    "        recon_loss = F.l1_loss(fake_mask.float(),true_mask.float())\n",
    "    \n",
    "    if loss == \"CE\":\n",
    "        N,C,H,W = fake_mask.shape\n",
    "        y_fake  = fake_mask.reshape(N,C,H*W)\n",
    "        y       = true_mask.reshape(N,H*W).long()\n",
    "        recon_loss = F.cross_entropy(y_fake,y)\n",
    "        \n",
    "    if loss == \"WCE\":\n",
    "        weights = [1.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
    "        weights = torch.tensor(weights).to(fake_mask.device)\n",
    "        weights = torch.softmax(weights, dim = 0)\n",
    "        N,C,H,W = fake_mask.shape\n",
    "        y_fake  = fake_mask.reshape(N,C,H*W)\n",
    "        y       = true_mask.reshape(N,H*W).long()\n",
    "        recon_loss = F.cross_entropy(y_fake,y, weight=weights)\n",
    "        \n",
    "    if loss == \"FL\":\n",
    "        #alpha, gamma = FL_params\n",
    "        N,C,H,W = fake_mask.shape\n",
    "        fake_mask = torch.softmax(fake_mask, dim=1) # Make values prob\n",
    "        y_fake  = fake_mask.reshape(N,C,H*W)\n",
    "        y       = true_mask.reshape(N,H*W).long()\n",
    "\n",
    "        recon_loss = focal_loss(y, y_fake)\n",
    "        \n",
    "    return adv_loss + lambda_*recon_loss\n",
    "\n",
    "\n",
    "def discriminator_loss(real_pred,fake_pred, slow_down = 2):\n",
    "    \n",
    "    real_labels = torch.ones_like(real_pred)\n",
    "    fake_labels = torch.zeros_like(fake_pred)\n",
    "    \n",
    "    \n",
    "    fake_loss = F.binary_cross_entropy(fake_pred,fake_labels)\n",
    "    real_loss = F.binary_cross_entropy(real_pred,real_labels)\n",
    "    \n",
    "    \n",
    "    #Divide loss with 2 to slow learning\n",
    "    return (real_loss + fake_loss)/slow_down\n",
    "    \n",
    "    \n",
    "def combined_loss(gen_loss, disc_loss, gen_weight=0.5, disc_weight=0.5):\n",
    "    return gen_weight * (gen_loss / (gen_loss + disc_loss)) + disc_weight * (disc_loss / (gen_loss + disc_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T10:30:12.337106Z",
     "iopub.status.busy": "2023-12-21T10:30:12.336294Z",
     "iopub.status.idle": "2023-12-21T10:30:41.183873Z",
     "shell.execute_reply": "2023-12-21T10:30:41.183056Z",
     "shell.execute_reply.started": "2023-12-21T10:30:12.337064Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = \"/kaggle/input/data-deloitte/Prossed_data_train_ny.npy\"\n",
    "\n",
    "\n",
    "generator_seed = torch.Generator().manual_seed(42)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()])\n",
    "dataset = CarSegData(data_root = PATH, transform = transform )\n",
    "# Get the total length of the dataset\n",
    "dataset_length = len(dataset)\n",
    "\n",
    "# Get the indices of the last 138 elements\n",
    "last_indices = range(dataset_length - 138, dataset_length)\n",
    "\n",
    "# Create a new dataset with only the last 138 elements\n",
    "dataset2 = Subset(dataset, last_indices)\n",
    "\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset2, [130, 8], generator = generator_seed)\n",
    "\n",
    "#train_set = CarSegData(data_root='/kaggle/input/10percent/train10.npy', transform=transform)\n",
    "#val_set = CarSegData(data_root='/kaggle/input/10percent/val_real.npy', transform=transform)\n",
    "#print(len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T10:30:41.185756Z",
     "iopub.status.busy": "2023-12-21T10:30:41.185470Z",
     "iopub.status.idle": "2023-12-21T10:30:44.361312Z",
     "shell.execute_reply": "2023-12-21T10:30:44.360537Z",
     "shell.execute_reply.started": "2023-12-21T10:30:41.185731Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "best_val = 1000\n",
    "best_mIoU = 0\n",
    "loss = \"FL\"\n",
    "\n",
    "\n",
    "wd = 0\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "LAMBDA= 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 9\n",
    "\n",
    "\n",
    "generator = UNet_Generator(device, out_channels = num_classes).to(device)\n",
    "discriminator = PatchGan_Discriminator(device).to(device)\n",
    "\n",
    "ge_loss = 0\n",
    "de_loss = 0\n",
    "\n",
    "train_loss_g = []\n",
    "train_loss_d = []\n",
    "IoU_loss = []\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "batch_size = BATCH_SIZE #paper 1\n",
    "\n",
    "\n",
    "num_workers = 2\n",
    "\n",
    "# Define Genertor and Discrimantor optimizers\n",
    "# Define separate learning rates\n",
    "gen_learning_rate  = 0.001    # adjust this value as needed\n",
    "disc_learning_rate = 0.00002  # adjust this value as needed\n",
    "\n",
    "# Define separate optimizers\n",
    "g_optim = torch.optim.Adam(generator.parameters(), lr=gen_learning_rate, betas=(0.5,0.999), weight_decay=wd) \n",
    "d_optim = torch.optim.Adam(discriminator.parameters(), lr=disc_learning_rate,betas=(0.5,0.999),  weight_decay=wd)\n",
    "\n",
    "mean_losses = np.zeros(shape = (3,num_epochs))\n",
    "mean_val_losses = np.zeros(shape = (2,num_epochs)) #Only generator loss and IoU\n",
    "\n",
    "\n",
    "metric = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n",
    "metric = metric.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_set,batch_size = batch_size, shuffle = True, worker_init_fn=42)\n",
    "val_loader = DataLoader(val_set,batch_size = batch_size, shuffle = True, worker_init_fn=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T10:41:55.354842Z",
     "iopub.status.busy": "2023-12-21T10:41:55.353933Z"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_g = []\n",
    "    epoch_loss_d = []\n",
    "    epoch_loss_IoU = []\n",
    "\n",
    "    start_time = time.time()  # Start time\n",
    "    for batch in train_loader:\n",
    "\n",
    "        img, mask = batch\n",
    "        img = img.to(device)\n",
    "        mask = mask.to(device)\n",
    "        mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "        #Train Discriminator\n",
    "\n",
    "        fake_mask = generator(img)\n",
    "\n",
    "        fake_pred = discriminator(fake_mask.detach(),img)\n",
    "        real_pred = discriminator(mask,img)\n",
    "\n",
    "        d_loss = discriminator_loss(real_pred,fake_pred)\n",
    "        #Update discriminator\n",
    "        d_optim.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optim.step()\n",
    "\n",
    "        #Train Generator\n",
    "        fake_pred = discriminator(fake_mask,img)\n",
    "        g_loss = generator_loss(fake_pred,fake_mask,mask,lambda_ = 100, loss = loss)\n",
    "\n",
    "        #Update Generator\n",
    "        g_optim.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "\n",
    "        ge_loss += g_loss.item()\n",
    "        de_loss += d_loss.item()\n",
    "\n",
    "        IoU = metric(masker(fake_mask),mask)\n",
    "\n",
    "        epoch_loss_g.append(g_loss.item())\n",
    "        epoch_loss_d.append(d_loss.item())\n",
    "        epoch_loss_IoU.append(IoU.item())\n",
    "\n",
    "        mean_losses[0][epoch] = np.mean(epoch_loss_g)\n",
    "        mean_losses[1][epoch] = np.mean(epoch_loss_d)\n",
    "        mean_losses[2][epoch] = np.mean(epoch_loss_IoU)\n",
    "\n",
    "\n",
    "        end_time = time.time()  # End time\n",
    "        epoch_time = end_time - start_time  # Time taken for the epoch\n",
    "\n",
    "    #Validation for each epoch\n",
    "    mIoU_val = []\n",
    "    val_loss = []\n",
    "    for img, mask in val_loader:\n",
    "        img = img.to(device)\n",
    "        mask = mask.to(device)\n",
    "        mask = mask.unsqueeze(1)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        #Get generator validation loss\n",
    "\n",
    "        fake_mask = generator(img)\n",
    "        fake_pred = discriminator(fake_mask.detach(),img)\n",
    "        g_loss_val = generator_loss(fake_pred,fake_mask,mask,lambda_ = 100, loss = loss)\n",
    "        \n",
    "        fake_mask = torch.argmax(softmax(fake_mask),axis=1)\n",
    "        fake_mask = fake_mask.unsqueeze(1)\n",
    "        IoU = metric(fake_mask,mask)\n",
    "    \n",
    "        mIoU_val.append(IoU.item())\n",
    "        val_loss.append(g_loss_val.item())\n",
    "    \n",
    "    mIoU_val = np.mean(mIoU_val)\n",
    "    val_loss = np.mean(val_loss)\n",
    "    \n",
    "    #Save model with lowest val loss\n",
    "    if val_loss < best_val:\n",
    "        model_val = {'epoch': epoch+1,\n",
    "                     'generator' : generator,\n",
    "                     'loss': val_loss,\n",
    "                     'IoU' : mIoU_val,\n",
    "                     'model_state_dict': generator.state_dict(),\n",
    "                     'optimizer_state_dict': g_optim.state_dict(),\n",
    "                     'loss_type': loss} \n",
    "        best_val = val_loss\n",
    "    #Save model with lowest val IoU\n",
    "    if mIoU_val > best_mIoU:\n",
    "        model_IoU = {'epoch': epoch+1,\n",
    "                     'generator' : generator,\n",
    "                     'loss': val_loss,\n",
    "                     'IoU' : mIoU_val,\n",
    "                     'model_state_dict': generator.state_dict(),\n",
    "                     'optimizer_state_dict': g_optim.state_dict(),\n",
    "                     'loss_type': loss} \n",
    "        best_mIoU = mIoU_val\n",
    "        \n",
    "        \n",
    "    mean_val_losses[0][epoch] = val_loss\n",
    "    mean_val_losses[1][epoch] = mIoU_val\n",
    "        \n",
    "        \n",
    "        \n",
    "    if epoch in list(range(0,num_epochs,5)):\n",
    "        print(\"Epoch: {:3d} | Time: {:5.2f}s | ge_loss: {:5.3f} | de_loss: {:5.3f} | IoU: {:5.3f} | Val ge_loss : {:5.3f} | Val IoU: {:5.3f}\".format(epoch, epoch_time,  mean_losses[0][epoch] , mean_losses[1][epoch], mean_losses[2][epoch],\n",
    "                                                                                                                                                     mean_val_losses[0][epoch], mean_val_losses[1][epoch]))\n",
    "\n",
    "\n",
    "#Save best models\n",
    "torch.save(model_val, \"generator_val_\"+ loss + \".pt\")\n",
    "torch.save(model_IoU, \"generator_IoU_\"+ loss + \".pt\")\n",
    "\n",
    "#Save final models\n",
    "torch.save(generator, \"generator_\"+ loss + \".pt\")\n",
    "torch.save(discriminator, \"discriminator_\"+ loss + \".pt\")\n",
    "torch.save(IoU_loss, \"IoU_FL.pt\")\n",
    "torch.save(mean_losses, \"mean_losses_\"+ loss + \".pt\")\n",
    "torch.save(mean_val_losses, \"mean_val_losses_\"+ loss + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, generator):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    #generator.eval()\n",
    "    mIoU = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).float()\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            z = generator(x)\n",
    "            preds = torch.argmax(softmax(z),axis=1)\n",
    "            IoU = metric(preds,y)\n",
    "            mIoU.append(IoU.item())\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(15, 15))\n",
    "            ax1.imshow(x[0,:,:,:].cpu().detach().numpy().transpose(1,2,0))\n",
    "            ax1.set_title('Original image')\n",
    "            ax1.axis('off')\n",
    "            ax2.imshow(y[0,:,:].cpu().detach().numpy(),cmap=cmap)\n",
    "            ax2.set_title('Targets')\n",
    "            ax2.axis('off')\n",
    "            ax3.imshow(preds[0,:,:].cpu().detach().numpy(),cmap=cmap)\n",
    "            ax3.set_title(f'Predictions, IoU: {IoU:.3}')\n",
    "            ax3.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T19:00:20.664389Z",
     "iopub.status.busy": "2023-12-20T19:00:20.664022Z",
     "iopub.status.idle": "2023-12-20T19:01:12.831860Z",
     "shell.execute_reply": "2023-12-20T19:01:12.830981Z",
     "shell.execute_reply.started": "2023-12-20T19:00:20.664358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the training/validation masks\n",
    "check_accuracy(train_loader, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(val_loader, generator)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4031150,
     "sourceId": 7011310,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4044812,
     "sourceId": 7031966,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4068111,
     "sourceId": 7065299,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4068114,
     "sourceId": 7065303,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4109296,
     "sourceId": 7123807,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4109728,
     "sourceId": 7124429,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4111472,
     "sourceId": 7127069,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4112301,
     "sourceId": 7128205,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4112789,
     "sourceId": 7128904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4203180,
     "sourceId": 7253938,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
